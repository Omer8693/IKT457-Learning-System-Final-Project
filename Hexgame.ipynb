{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a499d04-72f9-4b26-8d17-89d08559ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from GraphTsetlinMachine.tm import MultiClassGraphTsetlinMachine\n",
    "from GraphTsetlinMachine.graphs import Graphs\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "138b3745-ea1d-4958-9f34-b3b9b5681787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Default arguments and parameter initialization\n",
    "def default_args(**kwargs):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", default=100, type=int)\n",
    "    parser.add_argument(\"--board_size\", default=7, type=int) \n",
    "    parser.add_argument(\"--depth\", default=3, type=int)\n",
    "    parser.add_argument(\"--hypervector_size\", default=16, type=int)\n",
    "    parser.add_argument(\"--hypervector_bits\", default=2, type=int)\n",
    "    parser.add_argument(\"--message_size\", default=128, type=int)\n",
    "    parser.add_argument(\"--message_bits\", default=2, type=int)\n",
    "    parser.add_argument('--double_hashing', dest='double_hashing', default=False, action='store_true')\n",
    "    parser.add_argument(\"--max_included_literals\", default=16, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    if args.board_size == 5:\n",
    "        args.number_of_clauses = 20000\n",
    "        args.T = 10000\n",
    "        args.s = 100\n",
    "    elif args.board_size == 6:\n",
    "        args.number_of_clauses = 20000\n",
    "        args.T = 5000\n",
    "        args.s = 5\n",
    "    elif args.board_size == 7:\n",
    "        args.number_of_clauses = 7200\n",
    "        args.T = 3000\n",
    "        args.s = 1.2\n",
    "    elif args.board_size == 9:\n",
    "        args.number_of_clauses = 3200\n",
    "        args.T = 4000\n",
    "        args.s = 0.8\n",
    "    elif args.board_size == 11:\n",
    "        args.number_of_clauses = 20000\n",
    "        args.T = 10000\n",
    "        args.s = 1.0\n",
    "    elif args.board_size == 13:\n",
    "        args.number_of_clauses = 5000\n",
    "        args.T = 8000\n",
    "        args.s = 0.7\n",
    "\n",
    "    for key, value in kwargs.items():\n",
    "        if hasattr(args, key):\n",
    "            setattr(args, key, value)\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c353b0-74c2-4588-b4e2-d8f1ec2abd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data based on board size\n",
    "def load_data_for_board_size(board_size):\n",
    "    file_paths = {\n",
    "        5: \"IKT457-Learning-System-Final-Project/Data/5x5.csv\",\n",
    "        6: \"IKT457-Learning-System-Final-Project/Data/6x6.csv\",\n",
    "        7: \"IKT457-Learning-System-Final-Project/Data/7x7.csv\",        \n",
    "        9: \"IKT457-Learning-System-Final-Project/Data/9x9.csv\",\n",
    "        11: \"IKT457-Learning-System-Final-Project/Data/11x11.csv\",\n",
    "        13: \"IKT457-Learning-System-Final-Project/Data/13x13.csv\",\n",
    "    }\n",
    "    if board_size not in file_paths:\n",
    "        raise ValueError(f\"Unsupported board size: {board_size}\")\n",
    "\n",
    "    file_path = file_paths[board_size]\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "    node_names = [f\"{i}_{j}\" for i in range(1, board_size + 1) for j in range(1, board_size + 1)]\n",
    "    \n",
    "    # Check if all required columns are present\n",
    "    if not set(node_names + [\"Winner\"]).issubset(data.columns):\n",
    "        raise ValueError(f\"Missing required columns for board size {board_size}. Check your dataset.\")\n",
    "    \n",
    "    X = data[node_names].values\n",
    "    y = data[\"Winner\"].values.astype(int)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf987a-6a67-4602-b474-23655a2d6333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for node edges\n",
    "def position_to_edge_id(pos, board_size):\n",
    "    return pos[0] * board_size + pos[1]\n",
    "\n",
    "# Initialize arguments\n",
    "args = default_args()\n",
    "\n",
    "# Load data\n",
    "X, y = load_data_for_board_size(args.board_size)\n",
    "\n",
    "# Ensure labels have variation\n",
    "if len(np.unique(y)) <= 1:\n",
    "    raise ValueError(\"Labels must contain at least two classes for training.\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "split_idx = int(len(X) * 0.9)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Check label distribution\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "print(f\"Training class distribution: {dict(zip(unique_train, counts_train))}\")\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "print(f\"Testing class distribution: {dict(zip(unique_test, counts_test))}\")\n",
    "\n",
    "# Define edges dynamically based on board size\n",
    "edges = []\n",
    "for i in range(args.board_size):\n",
    "    for j in range(args.board_size):\n",
    "        if j < args.board_size - 1:\n",
    "            edges.append(((i, j), (i, j + 1)))\n",
    "        if i < args.board_size - 1:\n",
    "            edges.append(((i, j), (i + 1, j)))\n",
    "        if i < args.board_size - 1 and j > 0:\n",
    "            edges.append(((i, j), (i + 1, j - 1)))\n",
    "\n",
    "# Limit edges for nodes dynamically\n",
    "n_edges_list = []\n",
    "for i in range(args.board_size ** 2):\n",
    "    if i == 0 or i == args.board_size ** 2 - 1:\n",
    "        n_edges_list.append(2)\n",
    "    elif i == args.board_size - 1 or i == args.board_size ** 2 - args.board_size:\n",
    "        n_edges_list.append(3)\n",
    "    elif i // args.board_size == 0 or i // args.board_size == args.board_size - 1:\n",
    "        n_edges_list.append(4)\n",
    "    elif i % args.board_size == 0 or i % args.board_size == args.board_size - 1:\n",
    "        n_edges_list.append(4)\n",
    "    else:\n",
    "        n_edges_list.append(6)\n",
    "\n",
    "# Create training graphs\n",
    "graphs_train = Graphs(\n",
    "    number_of_graphs=len(X_train),\n",
    "    symbols=[\"O\", \"X\", \".\"],\n",
    "    hypervector_size=args.hypervector_size,\n",
    "    hypervector_bits=args.hypervector_bits,\n",
    "    double_hashing=args.double_hashing\n",
    ")\n",
    "\n",
    "for graph_id in range(X_train.shape[0]):\n",
    "    graphs_train.set_number_of_graph_nodes(graph_id, args.board_size ** 2)\n",
    "\n",
    "graphs_train.prepare_node_configuration()\n",
    "\n",
    "# Add nodes and properties to each graph\n",
    "for graph_id, board in enumerate(X_train):\n",
    "    for node_id in range(len(board)):\n",
    "        graphs_train.add_graph_node(graph_id, f\"node_{node_id}\", n_edges_list[node_id])\n",
    "        graphs_train.add_graph_node_property(graph_id, f\"node_{node_id}\", board[node_id])\n",
    "\n",
    "graphs_train.prepare_edge_configuration()\n",
    "\n",
    "# Add edges\n",
    "def add_edges(graphs, edges):\n",
    "    for graph_id in range(graphs.number_of_graphs):\n",
    "        for edge in edges:\n",
    "            src_id = position_to_edge_id(edge[0], args.board_size)\n",
    "            dest_id = position_to_edge_id(edge[1], args.board_size)\n",
    "            graphs.add_graph_node_edge(graph_id, f\"node_{src_id}\", f\"node_{dest_id}\", edge_type_name=\"Plain\")\n",
    "            graphs.add_graph_node_edge(graph_id, f\"node_{dest_id}\", f\"node_{src_id}\", edge_type_name=\"Plain\")\n",
    "\n",
    "add_edges(graphs_train, edges)\n",
    "graphs_train.encode()\n",
    "\n",
    "# Create testing graphs\n",
    "graphs_test = Graphs(len(X_test), init_with=graphs_train)\n",
    "\n",
    "for graph_id in range(X_test.shape[0]):\n",
    "    graphs_test.set_number_of_graph_nodes(graph_id, args.board_size ** 2)\n",
    "\n",
    "graphs_test.prepare_node_configuration()\n",
    "\n",
    "for graph_id, board in enumerate(X_test):\n",
    "    for node_id in range(len(board)):\n",
    "        graphs_test.add_graph_node(graph_id, f\"node_{node_id}\", n_edges_list[node_id])\n",
    "        graphs_test.add_graph_node_property(graph_id, f\"node_{node_id}\", board[node_id])\n",
    "\n",
    "graphs_test.prepare_edge_configuration()\n",
    "add_edges(graphs_test, edges)\n",
    "graphs_test.encode()\n",
    "\n",
    "# Train and evaluate the GTM\n",
    "tm = MultiClassGraphTsetlinMachine(\n",
    "    args.number_of_clauses,\n",
    "    args.T,\n",
    "    args.s,\n",
    "    depth=args.depth,\n",
    "    message_size=args.message_size,\n",
    "    message_bits=args.message_bits,\n",
    "    max_included_literals=args.max_included_literals\n",
    ")\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "start_training = time()\n",
    "for epoch in range(args.epochs):\n",
    "    tm.fit(graphs_train, y_train, epochs=1, incremental=True)\n",
    "    train_acc = np.mean(y_train == tm.predict(graphs_train))\n",
    "    test_acc = np.mean(y_test == tm.predict(graphs_test))\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    print(f\"Epoch {epoch + 1}: Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"Training time: {time() - start_training:.2f} seconds\")\n",
    "\n",
    "\n",
    "def print_clause_weights(tm):\n",
    "    weights = tm.get_state()[1].reshape(2, -1) \n",
    "    for i in range(tm.number_of_clauses):\n",
    "        print(f\"Clause #{i} - Weights: (Positive: {weights[0, i]}, Negative: {weights[1, i]})\", end=\" \")\n",
    "        literals = []\n",
    "        for k in range(tm.number_of_features * 2):\n",
    "            if tm.ta_action(0, i, k):\n",
    "                literals.append(f\"x{k}\" if k < tm.number_of_features else f\"NOT x{k - tm.number_of_features}\")\n",
    "        print(\" AND \".join(literals))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86e564-cf28-4bab-a538-f2e949d0106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.plot(range(1, len(test_accuracies) + 1), test_accuracies, label='Test Accuracy', marker='x')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Testing Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcaec64-d87d-4d5e-b4f9-08d99937916e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
